# üìà Der Nasdaq100 Experte: RAG-Chatbot f√ºr Finanzanalysen

Dieses Projekt ist eine interaktive **Streamlit-Webanwendung**, die einen hochentwickelten **RAG (Retrieval-Augmented Generation) Chatbot** bereitstellt. Der Bot agiert als spezialisierter Finanzanalyst und beantwortet Fragen basierend auf aktuellen und historischen Finanzdaten der Unternehmen des **Nasdaq 100 Index**.

Link: https://schelki.streamlit.app/

Die gesamte Logik des Chatbots wird durch den **LangGraph**-State-Machine-Ansatz gesteuert, um einen pr√§zisen, mehrstufigen Analyseprozess zu gew√§hrleisten.

## üåü Funktionen und Anwendungsfall

* **Faktengest√ºtzte Analyse:** Der Chatbot nutzt eine interne Datenbank (ChromaDB) mit Finanzkennzahlen, Gesch√§ftsberichten und aktuellen Nachrichten der Nasdaq 100-Unternehmen.
* **Historie & Kontext:** Durch die Verwendung von LangGraph kann der Bot den Chatverlauf ber√ºcksichtigen und Folgefragen ("Und was ist mit dieser Firma?") korrekt im Kontext beantworten (**Query Reformulation**).
* **Einfache Bedienung:** Intuitive Streamlit-Oberfl√§che f√ºr die Eingabe des OpenAI API-Keys und die direkte Interaktion.
* **Datengrundlage:** Die Analyse basiert auf der Datei `nasdaq_100_final_for_RAG.csv`.

---

## üíª Technischer √úberblick: Die RAG-Pipeline mit LangGraph

Die Anwendung folgt einem mehrstufigen LangGraph-Workflow, um jede Nutzeranfrage zu verarbeiten. Der Prozess ist darauf ausgelegt, die Genauigkeit und Relevanz der generierten Antworten zu maximieren. 

### 1. Initialisierung und Datenaufnahme (Caching)

Beim Start der Anwendung wird die Datenbank aufgebaut und dank `st.cache_resource` im Speicher gehalten, um schnelle Folgeanfragen zu erm√∂glichen:

1.  **Datenbereinigung:** Die CSV-Datei wird geladen, leere Zellen (`NaN`) werden gef√ºllt, und die Daten werden in den String-Typ umgewandelt.
2.  **Dokumenterstellung:** Jede Zeile der CSV wird in ein LangChain-`Document`-Objekt umgewandelt. Der Haupttext enth√§lt die Unternehmenszusammenfassung und News, w√§hrend **alle Finanzkennzahlen** in den **Metadaten** gespeichert werden.
3.  **Vektorisierung:** Die Dokumente werden in kleinere Chunks (Schnipsel) zerteilt (`RecursiveCharacterTextSplitter`), mithilfe von `OpenAIEmbeddings` in Vektoren umgewandelt und in einer **In-Memory ChromaDB** gespeichert.

### 2. LangGraph Workflow (Der Analyseprozess)

Jede Chat-Nachricht durchl√§uft diesen Graph:

| Knoten | Beschreibung |
| :--- | :--- |
| **`reformulate`** | **Pr√§zisierung der Frage:** Wenn ein Chatverlauf existiert, wird die aktuelle Nutzerfrage unter Ber√ºcksichtigung des Verlaufs umgeschrieben (z.B. "Wie ist deren KGV?" $\rightarrow$ "Wie ist das KGV von Apple?"). |
| **`retrieve`** | **Datensuche:** Der LangChain `Retriever` sucht in der ChromaDB nach den **Top 5** relevantesten Dokument-Chunks, die zur umformulierten Frage passen. |
| **`generate`** | **Antwortgenerierung:** Die gefundenen Dokumente (inklusive aller **Finanzkennzahlen aus den Metadaten**) werden zusammen mit der urspr√ºnglichen Frage und dem System-Prompt an das **LLM (GPT-3.5-Turbo)** √ºbergeben. Das LLM generiert die finale, faktenbasierte Antwort. |
| **`END`** | Der Workflow ist abgeschlossen. |

---

## üõ†Ô∏è Lokale Installation und Start

### Voraussetzungen

Sie ben√∂tigen:
* Python 3.9+
* Einen g√ºltigen **OpenAI API Key** (da die Modelle `text-embedding-3-small` und `gpt-3.5-turbo` verwendet werden).

### Einrichtung

1.  **Repository klonen:**
    ```bash
    git clone [IHR_REPO_LINK]
    cd [PROJEKT-ORDNER]
    ```

2.  **Abh√§ngigkeiten installieren:**
    ```bash
    pip install streamlit pandas openai langchain langchain-openai langchain-community langchain-core langgraph
    ```

3.  **Datenbankdatei:**
    Stellen Sie sicher, dass die CSV-Datei mit den Finanzdaten im Hauptverzeichnis des Projekts liegt:
    ```
    nasdaq_100_final_for_RAG.csv
    ```

4.  **Anwendung starten:**
    ```bash
    streamlit run [DATEINAME_DES_SKRIPTS].py
    ```

Nach dem Start wird die Anwendung im Browser ge√∂ffnet und fordert Sie in der Seitenleiste zur Eingabe Ihres **OpenAI API Keys** auf.
